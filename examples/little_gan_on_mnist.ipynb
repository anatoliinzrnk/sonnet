{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Yu7u1mNfnxVP"
      },
      "source": [
        "**Copyright 2019 The Sonnet Authors. All Rights Reserved.**\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YlYRfLOzyuW9"
      },
      "source": [
        "## Generative Adversarial Networks (GANs)\n",
        "\n",
        "In this notebook we'll use Sonnet 2 and TensorFlow 2 to train a small image generator using the Generative Adversarial Nets (GAN) [1] framework. GANs consist of two modules:\n",
        "\n",
        "1.   A **generator**, which takes randomly sampled noise or latents as inputs and produces data (in this case, images) as output.\n",
        "2.   A **discriminator**, which provides the learning signal for the generator. Its inputs are real images and generated images, and it's trained to predict whether each input is real or generated.  The generator is trained to \"fool\" the discriminator into believing its outputs are real.\n",
        "\n",
        "Typically both the generator and discriminator are deep neural networks.\n",
        "\n",
        "For an extended tutorial on GANs, see Ian Goodfellow's [GAN Tutorial](https://arxiv.org/abs/1701.00160) [2].\n",
        "\n",
        "[1] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. [Generative Adversarial Nets](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf). *NeurIPS*, 2014.\n",
        "\n",
        "[2] I. Goodfellow. [NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/abs/1701.00160). *arXiv:1701.00160*, 2017."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WAfR3cvnoGMB"
      },
      "source": [
        "# Preamble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4FqOAJb_jJR9"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "assert sys.version_info \u003e= (3, 6), \"Sonnet 2 requires Python \u003e=3.6\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ermIoeaTel6V"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import numpy as np\n",
        "import time\n",
        "try:\n",
        "  import sonnet.v2 as snt\n",
        "  import tensorflow.compat.v2 as tf\n",
        "  tf.enable_v2_behavior()\n",
        "  import tensorflow_datasets as tfds\n",
        "  import tqdm\n",
        "except ImportError:\n",
        "  !pip install tf-nightly-gpu-2.0-preview\n",
        "  !pip install tensorflow-datasets\n",
        "  !pip install git+https://github.com/deepmind/sonnet@v2\n",
        "  !pip install tqdm\n",
        "  import sonnet as snt\n",
        "  import tensorflow as tf\n",
        "  import tensorflow_datasets as tfds\n",
        "  import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Rpp_houJEHr9"
      },
      "outputs": [],
      "source": [
        "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "print(\"    Sonnet version: {}\".format(snt.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UYYmqvOKfNbk"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "We need to get our dataset in a state where we can iterate over it easily. The TensorFlow Datasets package provides a simple API for this. It will download the dataset and prepare it for us to speedily process on a GPU. We can also add our own pre-processing functions to mutate the dataset before our model sees it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UkBRriaQEr4z"
      },
      "outputs": [],
      "source": [
        "def process_batch(images, labels):\n",
        "  images = tf.squeeze(images, axis=[-1])\n",
        "  images = tf.cast(images, dtype=tf.float32)\n",
        "  images /= 255.\n",
        "  images = tf.clip_by_value(images, 0., 1.)\n",
        "  return images, labels\n",
        "\n",
        "batch_size = 100\n",
        "def mnist(split, batch_size=batch_size):\n",
        "  dataset, ds_info = tfds.load('mnist', split=split, as_supervised=True,\n",
        "                               with_info=True)\n",
        "  dataset = dataset.map(process_batch)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.cache()\n",
        "  return dataset, ds_info\n",
        "\n",
        "mnist_train, mnist_train_info = mnist('train')\n",
        "mnist_test, mnist_test_info = mnist('test')\n",
        "\n",
        "mnist_shuffled = mnist_train.shuffle(10000).repeat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JfOCWVGEfgcq"
      },
      "source": [
        "MNIST contains `28x28` greyscale handwritten digits. Let's take a look at one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "I_yM0TVjFCZq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "images, _ = next(iter(mnist_test))\n",
        "plt.imshow(images[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d7bsizs5gK3K"
      },
      "source": [
        "# Sonnet\n",
        "\n",
        "The next step is to define a model. In Sonnet everything that contains TensorFlow variables (`tf.Variable`) extends `snt.Module`, this includes low level neural network components (e.g. `snt.Linear`,  `snt.Conv2D`), larger nets containing subcomponents (e.g. `snt.nets.MLP`), optimizers (e.g. `snt.optimizers.Adam`) and whatever else you can think of.\n",
        "\n",
        "Modules provide a simple abstraction for storing parameters (and `Variable`s used for other purposes, like for storing moving averages in `BatchNorm`).\n",
        "\n",
        "To find all the parameters for a given module, simply do: `module.variables`. This will return a `tuple` of all the parameters that exist for this module, or any module it references:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GrN37pi1o4HT"
      },
      "source": [
        "## Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c6XoN56S2lSW"
      },
      "source": [
        "In Sonnet you build neural networks out of `snt.Module`s. In this simple example we'll build multi-layer perceptron (MLP) based generators and discriminators.\n",
        "\n",
        "We'll make use of \"Spectral Normalization\" [1] in both modules, which serves to regularize them.\n",
        "\n",
        "[1] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. [Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/abs/1802.05957). *arXiv:1802.05957*, 2018."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "hgjyB9yhFclD"
      },
      "outputs": [],
      "source": [
        "class SpectralNormalizer(snt.Module):\n",
        "\n",
        "  def __init__(self, epsilon=1e-12, name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.l2_normalize = functools.partial(tf.math.l2_normalize, epsilon=epsilon)\n",
        "\n",
        "  @snt.once\n",
        "  def _initialize(self, weights):\n",
        "    init = self.l2_normalize(snt.initializers.TruncatedNormal()(\n",
        "        shape=[1, weights.shape[-1]], dtype=weights.dtype))\n",
        "    # 'u' tracks our estimate of the first spectral vector for the given weight.\n",
        "    self.u = tf.Variable(init, name='u', trainable=False)\n",
        "\n",
        "  def __call__(self, weights, is_training=True):\n",
        "    self._initialize(weights)\n",
        "    if is_training:\n",
        "      # Do a power iteration and update u and weights.\n",
        "      weights_matrix = tf.reshape(weights, [-1, weights.shape[-1]])\n",
        "      v = self.l2_normalize(self.u @ tf.transpose(weights_matrix))\n",
        "      v_w = v @ weights_matrix\n",
        "      u = self.l2_normalize(v_w)\n",
        "      sigma = tf.stop_gradient(tf.reshape(v_w @ tf.transpose(u), []))\n",
        "      self.u.assign(u)\n",
        "      weights.assign(weights / sigma)\n",
        "    return weights\n",
        "\n",
        "\n",
        "class SpectrallyNormedLinear(snt.Linear):\n",
        "\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.spectral_normalizer = SpectralNormalizer()\n",
        "\n",
        "  def __call__(self, inputs, is_training=True):\n",
        "    self._initialize(inputs)\n",
        "\n",
        "    normed_w = self.spectral_normalizer(self.w, is_training=is_training)\n",
        "    outputs = tf.matmul(inputs, normed_w)\n",
        "    if self.with_bias:\n",
        "      outputs = tf.add(outputs, self.b)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "class SimpleBlock(snt.Module):\n",
        "\n",
        "  def __init__(self, embed_dim, with_batch_norm=False, name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.hidden = SpectrallyNormedLinear(self.embed_dim)\n",
        "    if with_batch_norm:\n",
        "      self.bn = snt.BatchNorm(create_scale=True, create_offset=True)\n",
        "    else:\n",
        "      self.bn = None\n",
        "\n",
        "  def __call__(self, inputs, is_training=True):\n",
        "    output = self.hidden(inputs, is_training=is_training)\n",
        "    if self.bn:\n",
        "      output = self.bn(output, is_training=is_training)\n",
        "    output = tf.nn.relu(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "class Generator(snt.Module):\n",
        "\n",
        "  def __init__(self, output_shape, num_layers=1, embed_dim=1024, name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.layers = [\n",
        "        SimpleBlock(embed_dim, with_batch_norm=True, name='block_'+str(index))\n",
        "        for index in range(num_layers)\n",
        "    ]\n",
        "    self.output_shape = tuple(output_shape)\n",
        "    output_size = np.prod(self.output_shape, dtype=int)\n",
        "    self.outputs = snt.Linear(output_size, name='outputs')\n",
        "\n",
        "  def __call__(self, inputs, is_training=True):\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    output = snt.Flatten()(inputs)\n",
        "    for layer in self.layers:\n",
        "      output = layer(output, is_training=is_training)\n",
        "    output = self.outputs(output)\n",
        "    output = tf.reshape(output, [-1] + list(self.output_shape))\n",
        "    output = tf.sigmoid(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "class Discriminator(snt.Module):\n",
        "\n",
        "  def __init__(self, num_layers=1, embed_dim=1024, name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.layers = [\n",
        "        SimpleBlock(embed_dim, with_batch_norm=False, name='block_'+str(index))\n",
        "        for index in range(num_layers)\n",
        "    ]\n",
        "    self.outputs = SpectrallyNormedLinear(1, name='outputs')\n",
        "\n",
        "  def __call__(self, inputs, is_training=True):\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    output = snt.Flatten()(inputs)\n",
        "    for layer in self.layers:\n",
        "      output = layer(output, is_training=is_training)\n",
        "    output = self.outputs(output)\n",
        "    return tf.reshape(output, [-1])\n",
        "\n",
        "\n",
        "class LittleGAN(snt.Module):\n",
        "\n",
        "  def __init__(self, num_layers=2, embed_dim=1024, name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.generator = Generator(\n",
        "        [28, 28], num_layers=num_layers, embed_dim=embed_dim)\n",
        "    self.discriminator = Discriminator(\n",
        "        num_layers=num_layers, embed_dim=embed_dim)\n",
        "\n",
        "  def generate(self, noise, is_training=True):\n",
        "    return self.generator(noise, is_training=is_training)\n",
        "\n",
        "  def discriminate(self, images):\n",
        "    return self.discriminator(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0i03px8y8gf7"
      },
      "source": [
        "Now we'll create an instance of our class whose weights will be randomly initialized. We'll train this MLP such that it learns to recognize digits in the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XqL8oIMqGAnU"
      },
      "outputs": [],
      "source": [
        "gan = LittleGAN(num_layers=2)\n",
        "gan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "snzkUUh9oXPy"
      },
      "source": [
        "## Using the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "On8wI6VwpDPm"
      },
      "source": [
        "Let's feed some random noise through the generator and see what it generates. Since the model is randomly initialized and not trained yet, the images it produces should look like noise.\n",
        "\n",
        "Below, the top row of images are real MNIST digits; the bottom row are the outputs of our randomly initialized generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4T-qmIc0GHfP"
      },
      "outputs": [],
      "source": [
        "images, labels = next(iter(mnist_test))\n",
        "noise_dim = 128\n",
        "\n",
        "def get_noise_batch(batch_size):\n",
        "  noise_shape = [batch_size, noise_dim]\n",
        "  return tf.random.normal(noise_shape, dtype=images.dtype)\n",
        "\n",
        "def get_label_batch(batch_size):\n",
        "  label_shape = [batch_size]\n",
        "  return tf.random.uniform(label_shape, maxval=10, dtype=labels.dtype)\n",
        "\n",
        "logits = gan.discriminate(images)\n",
        "noise = get_noise_batch(images.shape[0])\n",
        "gen_images = gan.generate(noise)\n",
        "\n",
        "num_images = 10\n",
        "plt.rcParams['figure.figsize'] = (2*num_images, 2)\n",
        "for i in range(num_images):\n",
        "  plt.subplot(2, num_images, i+1)\n",
        "  plt.imshow(images[i])\n",
        "  plt.subplot(2, num_images, num_images+i+1)\n",
        "  plt.imshow(gen_images[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9yV4JM345J71"
      },
      "source": [
        "Print the names, shapes, and other information about the variables in our little GAN model to get a rough idea of its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9ti32zve4_QG"
      },
      "outputs": [],
      "source": [
        "print(snt.format_variables(gan.variables))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V297xpzfobXK"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WTrv-jn4pPSx"
      },
      "source": [
        "To train the model we need a loss function for both the discriminator and generator, as well as an optimizer.\n",
        "\n",
        "We'll optimize the discriminator via the \"hinge loss\", defined by the function `hinge_loss_disc`. The generator simply maximizes the discriminator's error via a linear loss given by `loss_gen`.\n",
        "\n",
        "For the optimizer, we'll use the \"Adam\" optimizer (`snt.optimizers.Adam`). To compute gradients we'll use a `tf.GradientTape` which allows us to selectively record gradients only for the computation we want to back propagate through.\n",
        "\n",
        "We'll put all of this together below in a Sonnet Module called `GANOptimizer`, which takes as input the GAN to be optimized:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JgVCqok0usw-"
      },
      "outputs": [],
      "source": [
        "def hinge_loss_disc(preds_real, preds_gen):\n",
        "  loss_real = tf.reduce_mean(tf.nn.relu(1. - preds_real))\n",
        "  loss_gen = tf.reduce_mean(tf.nn.relu(1. + preds_gen))\n",
        "  return loss_real + loss_gen\n",
        "\n",
        "def loss_gen(preds_gen):\n",
        "  return -tf.reduce_mean(preds_gen)\n",
        "\n",
        "class GANOptimizer(snt.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               gan,\n",
        "               gen_batch_size=100,\n",
        "               disc_lr=2e-4,\n",
        "               gen_lr=5e-5,\n",
        "               loss_type='hinge',\n",
        "               num_epochs=100,\n",
        "               decay_lr_start_epoch=50,\n",
        "               decay_disc_lr=True,\n",
        "               decay_gen_lr=True,\n",
        "               name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.gan = gan\n",
        "    self.gen_batch_size = gen_batch_size\n",
        "    self.init_disc_lr = disc_lr\n",
        "    self.init_gen_lr = gen_lr\n",
        "    self.disc_lr = tf.Variable(\n",
        "        disc_lr, trainable=False, name='disc_lr', dtype=tf.float32)\n",
        "    self.gen_lr = tf.Variable(\n",
        "        gen_lr, trainable=False, name='gen_lr', dtype=tf.float32)\n",
        "    self.disc_opt = snt.optimizers.Adam(learning_rate=self.disc_lr, beta1=0.)\n",
        "    self.gen_opt = snt.optimizers.Adam(learning_rate=self.gen_lr, beta1=0.)\n",
        "    self.num_epochs = tf.constant(num_epochs, dtype=tf.int32)\n",
        "    self.decay_lr_start_epoch = tf.constant(decay_lr_start_epoch, dtype=tf.int32)\n",
        "    self.decay_disc_lr = decay_disc_lr\n",
        "    self.decay_gen_lr = decay_gen_lr\n",
        "\n",
        "  def disc_step(self, images, labels, lr_mult=1.):\n",
        "    \"\"\"Updates the discriminator once on the given batch of (images, labels).\"\"\"\n",
        "    del labels\n",
        "    gan = self.gan\n",
        "    with tf.GradientTape() as tape:\n",
        "      gen_images = gan.generate(get_noise_batch(images.shape[0]))\n",
        "      preds_real = gan.discriminate(images)\n",
        "      preds_gen = gan.discriminate(gen_images)\n",
        "      loss = hinge_loss_disc(preds_real, preds_gen)\n",
        "    disc_params = gan.discriminator.trainable_variables\n",
        "    disc_grads = tape.gradient(loss, disc_params)\n",
        "    if self.decay_disc_lr:\n",
        "      self.disc_lr.assign(self.init_disc_lr * lr_mult)\n",
        "    self.disc_opt.apply(disc_grads, disc_params)\n",
        "    return loss\n",
        "\n",
        "  def gen_step(self, lr_mult=1.):\n",
        "    \"\"\"Updates the generator once.\"\"\"\n",
        "    gan = self.gan\n",
        "    noise = get_noise_batch(self.gen_batch_size)\n",
        "    with tf.GradientTape() as tape:\n",
        "      gen_images = gan.generate(noise)\n",
        "      preds_gen = gan.discriminate(gen_images)\n",
        "      loss = loss_gen(preds_gen)\n",
        "    gen_params = gan.generator.trainable_variables\n",
        "    gen_grads = tape.gradient(loss, gen_params)\n",
        "    if self.decay_gen_lr:\n",
        "      self.gen_lr.assign(self.init_gen_lr * lr_mult)\n",
        "    self.gen_opt.apply(gen_grads, gen_params)\n",
        "    return loss\n",
        "\n",
        "  def _get_lr_mult(self, epoch):\n",
        "    # Linear decay to 0.\n",
        "    decay_epoch = tf.cast(epoch - self.decay_lr_start_epoch, tf.float32)\n",
        "    if decay_epoch \u003c tf.constant(0, dtype=tf.float32):\n",
        "      return tf.constant(1., dtype=tf.float32)\n",
        "    num_decay_epochs = tf.cast(self.num_epochs - self.decay_lr_start_epoch,\n",
        "                               dtype=tf.float32)\n",
        "    return (num_decay_epochs - decay_epoch) / num_decay_epochs\n",
        "\n",
        "  def step(self, train_batches, epoch):\n",
        "    \"\"\"Updates the discriminator and generator weights.\n",
        "\n",
        "    The discriminator is updated `len(train_batches)` times and the generator is\n",
        "    updated once.\n",
        "\n",
        "    Args:\n",
        "      train_batches: list of batches, where each item is an (image, label)\n",
        "        tuple. The discriminator is updated on each of these batches.\n",
        "      epoch: the epoch number, used to decide the learning rate multiplier for\n",
        "        learning rate decay.\n",
        "\n",
        "    Returns:\n",
        "      loss: the generator loss.\n",
        "      lr_mult: the computed learning rate multiplier.\n",
        "    \"\"\"\n",
        "    lr_mult = self._get_lr_mult(epoch)\n",
        "    for train_batch in train_batches:\n",
        "      self.disc_step(*train_batch, lr_mult=lr_mult)\n",
        "    return self.gen_step(lr_mult=lr_mult), lr_mult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UUkshshiK6Eq"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "num_epochs = 25\n",
        "num_disc_steps = 2\n",
        "\n",
        "# We'll turn the step function which updates our models into a tf.function using\n",
        "# autograph. This makes training much faster. If debugging, you can turn this\n",
        "# off by setting `debug = True`.\n",
        "debug = False\n",
        "\n",
        "optimizer = GANOptimizer(gan, num_epochs=25)\n",
        "step = optimizer.step\n",
        "if not debug:\n",
        "  step = tf.function(step)\n",
        "\n",
        "train_dataset = iter(mnist_shuffled)\n",
        "num_examples = mnist_train_info.splits['train'].num_examples\n",
        "total_batch_size_per_step = batch_size * num_disc_steps\n",
        "steps_per_epoch = num_examples // total_batch_size_per_step\n",
        "\n",
        "for epoch in tf.range(num_epochs):\n",
        "  print('Epoch = {} / {}'.format(epoch.numpy(), num_epochs))\n",
        "  start_time = time.time()\n",
        "  for _ in tqdm.tqdm(range(steps_per_epoch), position=0):\n",
        "    train_batches = [train_dataset.next() for _ in range(num_disc_steps)]\n",
        "    loss, lr_mult = step(train_batches, epoch)\n",
        "  elapsed = time.time() - start_time\n",
        "  print('Epoch = {} (lr_mult = {:0.02f}, loss = {}) done. ({} seconds)'.format(\n",
        "        epoch.numpy(), lr_mult.numpy(), loss.numpy(), elapsed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2K0_eoR8og-G"
      },
      "source": [
        "## Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cm_9RMJopgWc"
      },
      "source": [
        "Having trained our little GAN, let's check what its generated images look like now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PM7IPcOeXtxH"
      },
      "outputs": [],
      "source": [
        "num_images = 10\n",
        "noise = get_noise_batch(num_images)\n",
        "gen_images = gan.generate(noise)\n",
        "plt.rcParams['figure.figsize'] = (num_images, 1)\n",
        "for i in range(num_images):\n",
        "  plt.subplot(1, num_images, i+1)\n",
        "  plt.imshow(gen_images[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lnkc55PtqA_I"
      },
      "source": [
        "Whew, those should already look much closer to actual handwritten digits! But there's still a ways to go. There are many potential ways to improve the results further, including (roughly in order of implementation difficulty):\n",
        "\n",
        "* Train the model for more steps (e.g., try setting `num_epochs = 100` or higher).\n",
        "* Add learning rate decay to make the step size smaller after a certain amount of time. (For example, try `optimizer = GANOptimizer(num_epochs=100, decay_lr_start_epoch=50)` to begin decaying the learning rate halfway through training.)\n",
        "* Increase the size of the model -- make it deeper (`num_layers=4`) and/or wider (`embed_dim=2048`).\n",
        "* Tweak the learning hyperparameters, such as the discriminator \u0026 generator learning rates or the training batch size.\n",
        "* Make the models **convolutional**, using `snt.Conv2D` layers to process the inputs and outputs. The generator and discriminator defined above use MLPs, which naively treat the digit images as flat vectors.\n",
        "* Make the model **conditional** on labels by feeding the digit label (e.g., \"7\") into the generator and discriminator. (See [1, 2] for some ideas on how to do this.)\n",
        "\n",
        "[1] M. Mirza and S. Osindero. [*Conditional Generative Adversarial Networks*](https://arxiv.org/abs/1411.1784). *arXiv:1411.1784*, 2014.\n",
        "\n",
        "[2] T. Miyato and M. Koyama. [*cGANs with Projection Discriminator*](https://arxiv.org/abs/1802.05637). *arXiv:1802.05637*, 2018."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3",
        "kind": "private"
      },
      "name": "Sonnet 2 \"Little GAN\"",
      "provenance": [
        {
          "file_id": "",
          "timestamp": 1562258201721
        }
      ],
      "toc_visible": true,
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
