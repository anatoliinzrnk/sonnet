<!-- This file is machine generated: DO NOT EDIT THIS FILE.
     Edits will be overwritten the next time the file is generated. -->
<!-- common_typos_disable -->

# v2 - module reference
Sonnet built for TensorFlow 2.

## Other Functions and Classes
### [`class AxisNorm`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/axis_norm.py?q=class:AxisNorm){#AxisNorm .code-reference}

Normalizes inputs along the given axes.

This is a generic implementation of normalization along specific axes of the
input. `LayerNorm` and `InstanceNorm` are subclasses of this module, they
normalize over the channel and spatial dimensions respectively.

It transforms the input x into:

    outputs = scale * (x - mu) / (sigma + eps) + offset

Where `mu` and `sigma` are respectively the mean and standard deviation of
`x`.

There are many different variations for how users want to manage scale and
offset if they require them at all. These are:

  - No scale/offset in which case create_* should be set to False and
    scale/offset aren't passed when the module is called.
  - Trainable scale/offset in which case create_* should be set to True and
    again scale/offset aren't passed when the module is called. In this case
    this module creates and owns the scale/offset variables.
  - Externally generated scale/offset, such as for conditional normalization,
    in which case create_* should be set to False and then the values fed in
    at call time.

Attributes:
  scale: If `create_scale`, a trainable variable holding the current scale
    after the module is connected for the first time.
  offset: If `create_offset`, a trainable variable holding the current offset
    after the module is connected for the first time.

#### [`AxisNorm.__init__(axis, create_scale, create_offset, eps=0.0001, scale_init=None, offset_init=None, data_format='channels_last', name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/axis_norm.py?l=65){#AxisNorm.__init__ .code-reference}

Constructs an `AxisNorm` module.

##### Args:


* `axis`: An int, slice or sequence of ints representing the axes which should
    be normalized across.
* `create_scale`: Boolean representing whether to create a trainable scale per
    channel applied after the normalization.
* `create_offset`: Boolean representing whether to create a trainable offset
    per channel applied after normalization and scaling.
* `eps`: Small epsilon to avoid division by zero variance. Defaults to 1e-4.
* `scale_init`: Optional initializer for the scale variable. Can only be set
    if `create_scale` is True. By default scale is initialized to one.
* `offset_init`: Optional initializer for the offset variable. Can only be set
    if `create_offset` is True. By default offset is initialized to zero.
* `data_format`: The data format of the input. Can be either `channels_first`,
    `channels_last`, `N...C` or `NC...`. By default it is `channels_last`.
* `name`: Name of the module.


#### [`AxisNorm.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#AxisNorm.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`AxisNorm.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#AxisNorm.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`AxisNorm.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#AxisNorm.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`AxisNorm.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#AxisNorm.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`AxisNorm.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#AxisNorm.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`AxisNorm.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#AxisNorm.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class BaseBatchNorm`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/batch_norm.py?q=class:BaseBatchNorm){#BaseBatchNorm .code-reference}

Batch normalization module.

This implements normalization across the batch and spatial dimensions.
It maintains moving averages of the mean and variance which can be
used to normalize at test time. The constructor is generic and
requires the user to pass in objects to compute these.

At training time we use the batch statistics for that batch and these are then
used to update the moving averages.

At test time we can either use the moving averages of the batch statistics
(`test_local_stats=False`) or we can use the local statistics
(`test_local_stats=True`).

It transforms the input x into:

    outputs = scale * (x - mu) / (sigma + eps) + offset

Where `mu` and `sigma` are respectively the mean and standard deviation of
`x`. Note that this module automatically uses the fused batch norm op if the
data format is `NHWC`.

There are many different variations for how users want to manage scale and
offset if they require them at all. These are:

  - No scale/offset in which case create_* should be set to False and
    scale/offset aren't passed when the module is called.
  - Trainable scale/offset in which case create_* should be set to True and
    again scale/offset aren't passed when the module is called. In this case
    this module creates and owns the scale/offset variables.
  - Externally generated scale/offset, such as for conditional normalization,
    in which case create_* should be set to False and then the values fed in
    at call time.

Attributes:
  scale: If `create_scale`, a trainable variable holding the current scale
    after the module is connected for the first time.
  offset: If `create_offset`, a trainable variable holding the current offset
    after the module is connected for the first time.

#### [`BaseBatchNorm.__init__(create_scale, create_offset, moving_mean, moving_variance, eps=0.0001, scale_init=None, offset_init=None, data_format='channels_last', name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/batch_norm.py?l=72){#BaseBatchNorm.__init__ .code-reference}

Constructs a `BaseBatchNorm` module.

##### Args:


* `create_scale`: Boolean representing whether to create a trainable scale per
    channel applied after the normalization.
* `create_offset`: Boolean representing whether to create a trainable offset
    per channel applied after normalization and scaling.
* `moving_mean`: An object which keeps track of the moving average of the mean
    which can be used to normalize at test time. This object must have an
    update method which takes a value and updates the internal state and a
    value property which returns the current mean.
* `moving_variance`: An object which keeps track of the moving average of the
    variance which can be used to normalize at test time. This object must
    have an update method which takes a value and updates the internal state
    and a value property which returns the current variance.
* `eps`: Small epsilon to avoid division by zero variance. Defaults to 1e-4.
* `scale_init`: Optional initializer for the scale variable. Can only be set
    if `create_scale` is True. By default scale is initialized to one.
* `offset_init`: Optional initializer for the offset variable. Can only be set
    if `create_offset` is True. By default offset is initialized to zero.
* `data_format`: The data format of the input. Can be either `channels_first`,
    `channels_last`, `N...C` or `NC...`. By default it is `channels_last`.
* `name`: Name of the module.


#### [`BaseBatchNorm.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#BaseBatchNorm.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`BaseBatchNorm.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#BaseBatchNorm.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`BaseBatchNorm.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#BaseBatchNorm.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`BaseBatchNorm.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#BaseBatchNorm.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`BaseBatchNorm.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#BaseBatchNorm.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`BaseBatchNorm.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#BaseBatchNorm.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class BatchNorm`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/batch_norm.py?q=class:BatchNorm){#BatchNorm .code-reference}

Batch normalization with exponential moving average for test statistics.

See `BaseBatchNorm` for details.

Attributes:
  scale: If `create_scale`, a trainable variable holding the current scale
    after the module is connected for the first time.
  offset: If `create_offset`, a trainable variable holding the current offset
    after the module is connected for the first time.

#### [`BatchNorm.__init__(create_scale, create_offset, decay_rate=0.999, eps=0.0001, scale_init=None, offset_init=None, data_format='channels_last', name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/batch_norm.py?l=249){#BatchNorm.__init__ .code-reference}

Constructs a `BatchNorm` module.

##### Args:


* `create_scale`: Boolean representing whether to create a trainable scale per
    channel applied after the normalization.
* `create_offset`: Boolean representing whether to create a trainable offset
    per channel applied after normalization and scaling.
* `decay_rate`: Decay rate of the exponential moving averages of the mean
    and variance.
* `eps`: Small epsilon to avoid division by zero variance. Defaults to 1e-4.
* `scale_init`: Optional initializer for the scale variable. Can only be set
    if `create_scale` is True. By default scale is initialized to one.
* `offset_init`: Optional initializer for the offset variable. Can only be set
    if `create_offset` is True. By default offset is initialized to zero.
* `data_format`: The data format of the input. Can be either `channels_first`,
    `channels_last`, `N...C` or `NC...`. By default it is `channels_last`.
* `name`: Name of the module.


#### [`BatchNorm.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#BatchNorm.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`BatchNorm.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#BatchNorm.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`BatchNorm.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#BatchNorm.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`BatchNorm.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#BatchNorm.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`BatchNorm.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#BatchNorm.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`BatchNorm.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#BatchNorm.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class Bias`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/bias.py?q=class:Bias){#Bias .code-reference}

Bias module.

Example Usage:

    >>> N, H, W, C = 1, 2, 3, 4
    >>> x = tf.random.normal([N, H, W, C])

    >>> scalar_bias = Bias(bias_dims=[])
    >>> scalar_bias_output = scalar_bias(x)
    >>> assert scalar_bias.b.shape == []

Create a bias over all non-minibatch dimensions:

    >>> all_bias = Bias()
    >>> all_bias_output = all_bias(x)
    >>> assert all_bias.b.shape == [H, W, C]

Create a bias over the last non-minibatch dimension:

    >>> last_bias = Bias(bias_dims=[-1])
    >>> last_bias_output = last_bias(x)
    >>> assert last_bias.b.shape == [C]

Create a bias over the first non-minibatch dimension:

    >>> first_bias = Bias(bias_dims=[1])
    >>> first_bias_output = first_bias(x)
    >>> assert first_bias.b.shape == [H, 1, 1]

Subtract and later add the same learned bias:

    >>> bias = Bias()
    >>> h1 = bias(x, multiplier=-1)
    >>> h2 = bias(x)
    >>> h3 = bias(x, multiplier=-1)
    >>> reconstructed_x = bias(h3)
    >>> assert tf.reduce_all(tf.equal(x, reconstructed_x))

#### [`Bias.__init__(output_size=None, bias_dims=None, b_init=None, name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/bias.py?l=69){#Bias.__init__ .code-reference}

Constructs a `Bias` module that supports broadcasting.

##### Args:


* `output_size`: Output size (output shape without batch dimension). If
      `output_size` is left as `None`, the size will be directly inferred
      by the input.
* `bias_dims`: Sequence of which dimensions to retain from the input shape
      when constructing the bias. The remaining dimensions will get
      broadcasted over (given size of 1), and leading dimensions will be
      removed completely. See class doc for examples.
* `b_init`: Optional initializer for the bias. Default to zeros.
* `name`: Name of the module.


#### [`Bias.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#Bias.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`Bias.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#Bias.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`Bias.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#Bias.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`Bias.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#Bias.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Bias.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#Bias.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Bias.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#Bias.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class Conv1D`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/conv.py?q=class:Conv1D){#Conv1D .code-reference}

`Conv1D` module.

#### [`Conv1D.__init__(output_channels, kernel_shape, stride=1, rate=1, padding='SAME', with_bias=True, w_init=None, b_init=None, data_format='NWC', name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/conv.py?l=171){#Conv1D.__init__ .code-reference}




#### [`Conv1D.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#Conv1D.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`Conv1D.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#Conv1D.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`Conv1D.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#Conv1D.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`Conv1D.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#Conv1D.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Conv1D.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#Conv1D.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Conv1D.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#Conv1D.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class Conv1DLSTM`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?q=class:Conv1DLSTM){#Conv1DLSTM .code-reference}

See `_ConvNDLSTM`.

#### [`Conv1DLSTM.__init__(input_shape, output_channels, kernel_shape, data_format='NWC', w_i_init=None, w_h_init=None, b_init=None, forget_bias=1.0, dtype=tf.float32, name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=1198){#Conv1DLSTM.__init__ .code-reference}




#### [`Conv1DLSTM.hidden_to_hidden`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=67){#Conv1DLSTM.hidden_to_hidden .code-reference}




#### [`Conv1DLSTM.initial_state(*args, **kwargs)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=60){#Conv1DLSTM.initial_state .code-reference}

See base class.


#### [`Conv1DLSTM.input_to_hidden`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=67){#Conv1DLSTM.input_to_hidden .code-reference}




#### [`Conv1DLSTM.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#Conv1DLSTM.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`Conv1DLSTM.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#Conv1DLSTM.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`Conv1DLSTM.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#Conv1DLSTM.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`Conv1DLSTM.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#Conv1DLSTM.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Conv1DLSTM.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#Conv1DLSTM.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Conv1DLSTM.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#Conv1DLSTM.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class Conv1DTranspose`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/conv_transpose.py?q=class:Conv1DTranspose){#Conv1DTranspose .code-reference}

`Conv1DTranspose` module.

#### [`Conv1DTranspose.__init__(output_channels, kernel_shape, output_shape=None, stride=1, rate=1, padding='SAME', with_bias=True, w_init=None, b_init=None, data_format='NWC', name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/conv_transpose.py?l=192){#Conv1DTranspose.__init__ .code-reference}




#### [`Conv1DTranspose.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#Conv1DTranspose.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`Conv1DTranspose.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#Conv1DTranspose.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`Conv1DTranspose.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#Conv1DTranspose.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`Conv1DTranspose.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#Conv1DTranspose.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Conv1DTranspose.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#Conv1DTranspose.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Conv1DTranspose.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#Conv1DTranspose.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class Conv2D`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/conv.py?q=class:Conv2D){#Conv2D .code-reference}

`Conv2D` module.

#### [`Conv2D.__init__(output_channels, kernel_shape, stride=1, rate=1, padding='SAME', with_bias=True, w_init=None, b_init=None, data_format='NHWC', name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/conv.py?l=198){#Conv2D.__init__ .code-reference}




#### [`Conv2D.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#Conv2D.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`Conv2D.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#Conv2D.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`Conv2D.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#Conv2D.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`Conv2D.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#Conv2D.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Conv2D.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#Conv2D.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Conv2D.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#Conv2D.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class Conv2DLSTM`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?q=class:Conv2DLSTM){#Conv2DLSTM .code-reference}

See `_ConvNDLSTM`.

#### [`Conv2DLSTM.__init__(input_shape, output_channels, kernel_shape, data_format='NHWC', w_i_init=None, w_h_init=None, b_init=None, forget_bias=1.0, dtype=tf.float32, name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=1227){#Conv2DLSTM.__init__ .code-reference}




#### [`Conv2DLSTM.hidden_to_hidden`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=67){#Conv2DLSTM.hidden_to_hidden .code-reference}




#### [`Conv2DLSTM.initial_state(*args, **kwargs)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=60){#Conv2DLSTM.initial_state .code-reference}

See base class.


#### [`Conv2DLSTM.input_to_hidden`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=67){#Conv2DLSTM.input_to_hidden .code-reference}




#### [`Conv2DLSTM.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#Conv2DLSTM.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`Conv2DLSTM.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#Conv2DLSTM.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`Conv2DLSTM.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#Conv2DLSTM.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`Conv2DLSTM.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#Conv2DLSTM.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Conv2DLSTM.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#Conv2DLSTM.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Conv2DLSTM.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#Conv2DLSTM.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class Conv2DTranspose`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/conv_transpose.py?q=class:Conv2DTranspose){#Conv2DTranspose .code-reference}

`Conv2DTranspose` module.

#### [`Conv2DTranspose.__init__(output_channels, kernel_shape, output_shape=None, stride=1, rate=1, padding='SAME', with_bias=True, w_init=None, b_init=None, data_format='NHWC', name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/conv_transpose.py?l=221){#Conv2DTranspose.__init__ .code-reference}




#### [`Conv2DTranspose.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#Conv2DTranspose.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`Conv2DTranspose.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#Conv2DTranspose.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`Conv2DTranspose.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#Conv2DTranspose.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`Conv2DTranspose.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#Conv2DTranspose.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Conv2DTranspose.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#Conv2DTranspose.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Conv2DTranspose.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#Conv2DTranspose.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class Conv3D`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/conv.py?q=class:Conv3D){#Conv3D .code-reference}

`Conv3D` module.

#### [`Conv3D.__init__(output_channels, kernel_shape, stride=1, rate=1, padding='SAME', with_bias=True, w_init=None, b_init=None, data_format='NDHWC', name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/conv.py?l=225){#Conv3D.__init__ .code-reference}




#### [`Conv3D.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#Conv3D.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`Conv3D.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#Conv3D.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`Conv3D.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#Conv3D.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`Conv3D.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#Conv3D.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Conv3D.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#Conv3D.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Conv3D.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#Conv3D.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class Conv3DLSTM`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?q=class:Conv3DLSTM){#Conv3DLSTM .code-reference}

See `_ConvNDLSTM`.

#### [`Conv3DLSTM.__init__(input_shape, output_channels, kernel_shape, data_format='NDHWC', w_i_init=None, w_h_init=None, b_init=None, forget_bias=1.0, dtype=tf.float32, name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=1256){#Conv3DLSTM.__init__ .code-reference}




#### [`Conv3DLSTM.hidden_to_hidden`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=67){#Conv3DLSTM.hidden_to_hidden .code-reference}




#### [`Conv3DLSTM.initial_state(*args, **kwargs)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=60){#Conv3DLSTM.initial_state .code-reference}

See base class.


#### [`Conv3DLSTM.input_to_hidden`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=67){#Conv3DLSTM.input_to_hidden .code-reference}




#### [`Conv3DLSTM.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#Conv3DLSTM.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`Conv3DLSTM.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#Conv3DLSTM.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`Conv3DLSTM.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#Conv3DLSTM.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`Conv3DLSTM.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#Conv3DLSTM.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Conv3DLSTM.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#Conv3DLSTM.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Conv3DLSTM.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#Conv3DLSTM.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class Conv3DTranspose`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/conv_transpose.py?q=class:Conv3DTranspose){#Conv3DTranspose .code-reference}

`Conv3DTranspose` module.

#### [`Conv3DTranspose.__init__(output_channels, kernel_shape, output_shape=None, stride=1, rate=1, padding='SAME', with_bias=True, w_init=None, b_init=None, data_format='NDHWC', name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/conv_transpose.py?l=250){#Conv3DTranspose.__init__ .code-reference}




#### [`Conv3DTranspose.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#Conv3DTranspose.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`Conv3DTranspose.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#Conv3DTranspose.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`Conv3DTranspose.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#Conv3DTranspose.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`Conv3DTranspose.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#Conv3DTranspose.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Conv3DTranspose.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#Conv3DTranspose.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Conv3DTranspose.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#Conv3DTranspose.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class DeepRNN`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?q=class:DeepRNN){#DeepRNN .code-reference}

Linear chain of modules or callables.

The core takes `(input, prev_state)` as input and passes the input
through each internal module in the order they were presented, using
elements from `prev_state` as necessary for internal RNN cores.

    >>> deep_rnn = snt.DeepRNN([
    ...     snt.LSTM(hidden_size=16),
    ...     snt.LSTM(hidden_size=16),
    ... ])

Note that the state of a `DeepRNN` is always a tuple, which will contain
the same number of elements as there are internal RNN cores. If no
internal modules are RNN cores, the state of the `DeepRNN` as a whole is
an empty tuple.

Wrapping non-recurrent modules into a `DeepRNN` can be useful to produce
something API compatible with a "real" recurrent module, simplifying
code that handles the cores.

#### [`DeepRNN.__init__(layers, name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=585){#DeepRNN.__init__ .code-reference}




#### [`DeepRNN.initial_state(*args, **kwargs)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=60){#DeepRNN.initial_state .code-reference}

See base class.


#### [`DeepRNN.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#DeepRNN.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`DeepRNN.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#DeepRNN.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`DeepRNN.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#DeepRNN.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`DeepRNN.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#DeepRNN.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`DeepRNN.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#DeepRNN.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`DeepRNN.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#DeepRNN.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class Deferred`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/deferred.py?q=class:Deferred){#Deferred .code-reference}

Defers the construction of another module until the first call.

Deferred can be used to declare modules that depend on computed properties of
other modules before those modules are defined. This allows users to separate
the declaration and use of modules. For example at the start of your program
you can declare two modules which are coupled:

    >>> encoder = snt.Linear(64)
    >>> decoder = snt.Deferred(lambda: snt.Linear(encoder.input_size))

Later you can use these naturally (note: that using `decoder` first would
cause an error since `encoder.input_size` is only defined after `encoder` has
been called):

    >>> x = tf.ones([8, 32])
    >>> y = encoder(x)
    >>> z = decoder(y)  # Constructs the Linear encoder by calling the lambda.

The result will satisfy the following conditions:

    >>> assert x.shape == z.shape
    >>> assert y.shape == [8, 64]
    >>> assert decoder.input_size == encoder.output_size
    >>> assert decoder.output_size == encoder.input_size

#### [`Deferred.__init__(constructor, call_methods=('__call__',), name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/deferred.py?l=52){#Deferred.__init__ .code-reference}

Initializes the `Deferred` module.

##### Args:


* `constructor`: A no argument callable which constructs the module to defer
    to. The first time one of the `call_methods` are called the constructor
    will be run and then the constructed module will be called with the same
    method and arguments as the deferred module.
* `call_methods`: Methods which should trigger construction of the target
    module. The default value configures this module to construct the first
    time `__call__` is run. If you want to add methods other than call you
    should explicitly pass them (optionally), for example
    `call_methods=("__call__", "encode", "decode")`.
* `name`: Name for the deferred module.


#### [`Deferred.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#Deferred.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`Deferred.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#Deferred.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`Deferred.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#Deferred.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`Deferred.target`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/deferred.py?l=80){#Deferred.target .code-reference}

Returns the target module.

If the constructor has not already run this will trigger construction.
Subsequent calls to `target` will return the same instance.

##### Returns:

  A `Module` instance as created by `self.constructor()` .


#### [`Deferred.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#Deferred.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Deferred.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#Deferred.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Deferred.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#Deferred.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class Dropout`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/dropout.py?q=class:Dropout){#Dropout .code-reference}

Randomly drop units in the input at a given rate.

See: http://www.cs.toronto.edu/~hinton/absps/dropout.pdf

Dropout was originally described by Hinton et al. TensorFlow deviates slightly
from this paper by scaling activations at training time rather than test time.

#### [`Dropout.__init__(rate, noise_shape=None, seed=None, name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/dropout.py?l=35){#Dropout.__init__ .code-reference}

Constructs a Dropout module.

##### Args:


* `rate`: Probability that each element of x is discarded. Must be a scalar in
    the range `[0, 1)`.
* `noise_shape`: (Optional) Shape vector controlling the shape of the random
    noise used to apply dropout. If not set this will be the shape of the
    input. If set it should be broadcastable to the input shape.
* `seed`: (Optional) Random seed to be passed to TensorFlow ops when
    generating dropout tensor.
* `name`: (Optional) Name for this module.


#### [`Dropout.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#Dropout.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`Dropout.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#Dropout.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`Dropout.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#Dropout.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`Dropout.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#Dropout.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Dropout.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#Dropout.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Dropout.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#Dropout.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class Flatten`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/reshape.py?q=class:Flatten){#Flatten .code-reference}

Flattens the input Tensor, preserving the batch dimension(s).

`Flatten` reshapes input tensors to combine all trailing dimensions apart
from the first. Additional leading dimensions can be preserved by setting the
`preserve_dims` parameter.

See `snt.Reshape` for more details.

#### [`Flatten.__init__(preserve_dims=1, name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/reshape.py?l=244){#Flatten.__init__ .code-reference}

Constructs a Flatten module.

##### Args:


* `preserve_dims`: Number of leading dimensions that will not be reshaped.
* `name`: Name of the module.


#### [`Flatten.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#Flatten.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`Flatten.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#Flatten.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`Flatten.reversed(name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/reshape.py?l=223){#Flatten.reversed .code-reference}

Returns inverse batch reshape.


#### [`Flatten.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#Flatten.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`Flatten.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#Flatten.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Flatten.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#Flatten.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Flatten.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#Flatten.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class GRU`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?q=class:GRU){#GRU .code-reference}

Gated recurrent unit (GRU) RNN core.

The implementation is based on: https://arxiv.org/abs/1412.3555.
Given x_t and previous hidden state h_{t-1} the core computes

    z_t = sigm(W_{iz} x_t + W_{hz} h_{t-1} + b_z)
    r_t = sigm(W_{ir} x_t + W_{hr} h_{t-1} + b_r)
    a_t = tanh(W_{ia} x_t + W_{ha} (r_t h_{t-1}) + b_a)
    h_t = (1 - z_t) h_{t-1} + z_t a_t

where `z_t` and `r_t` are reset and update gates.

Variables:
  w_i: input-to-hidden weights W_{iz}, W_{ir} and W_{ia} concatenated
    into a Tensor of shape [input_size, 3 * hidden_size].
  w_h: hidden-to-hidden weights W_{hz}, W_{hr} and W_{ha} concatenated
    into a Tensor of shape [hidden_size, 3 * hidden_size].
  b: biases b_z, b_r and b_a concatenated into a Tensor of shape
    [3 * hidden_size].

#### [`GRU.__init__(hidden_size, w_i_init=None, w_h_init=None, b_init=None, dtype=tf.float32, name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=1304){#GRU.__init__ .code-reference}

Construct a `GRU`.

##### Args:


* `hidden_size`: Hidden layer size.
* `w_i_init`: Optional initializer for the input-to-hidden weights.
    Defaults to Glorot uniform initializer.
* `w_h_init`: Optional initializer for the hidden-to-hidden weights.
    Defaults to Glorot uniform initializer.
* `b_init`: Optional initializer for the biases. Defaults to `Zeros`.
* `dtype`: Optional `tf.DType` of the core's variables. Defaults to
    `tf.float32`.
* `name`: Name of the module.


#### [`GRU.hidden_to_hidden`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=67){#GRU.hidden_to_hidden .code-reference}




#### [`GRU.initial_state(*args, **kwargs)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=60){#GRU.initial_state .code-reference}

See base class.


#### [`GRU.input_to_hidden`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=67){#GRU.input_to_hidden .code-reference}




#### [`GRU.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#GRU.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`GRU.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#GRU.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`GRU.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#GRU.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`GRU.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#GRU.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`GRU.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#GRU.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`GRU.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#GRU.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class GroupNorm`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/group_norm.py?q=class:GroupNorm){#GroupNorm .code-reference}

Constructs a group norm module.

This applies group normalization to the inputs. This involves splitting the
channels into groups before calculating the mean and variance. The default
behaviour is to compute the mean and variance over the spatial dimensions and
the grouped channels. The mean and variance will never be computed over the
created groups axis.

It transforms the input x into:

    outputs = scale * (x - mu) / sigma + offset

Where `mu` and `sigma` are respectively the mean and standard deviation of
`x`.

There are many different variations for how users want to manage scale and
offset if they require them at all. These are:

  - No scale/offset in which case create_* should be set to False and
    scale/offset aren't passed when the module is called.
  - Trainable scale/offset in which case create_* should be set to True and
    again scale/offset aren't passed when the module is called. In this case
    this module creates and owns the scale/offset variables.
  - Externally generated scale/offset, such as for conditional normalization,
    in which case create_* should be set to False and then the values fed in
    at call time.

Attributes:
  scale: If `create_scale`, a trainable variable holding the current scale
    after the module is connected for the first time.
  offset: If `create_offset`, a trainable variable holding the current offset
    after the module is connected for the first time.

#### [`GroupNorm.__init__(groups, axis=slice(1, None, None), create_scale=True, create_offset=True, eps=0.0001, scale_init=None, offset_init=None, data_format='channels_last', name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/group_norm.py?l=67){#GroupNorm.__init__ .code-reference}

Constructs a `GroupNorm` module.

##### Args:


* `groups`: An int, the number of groups to divide the channels by. The number
    of channels must be divisible by this.
* `axis`: An int, slice or sequence of ints representing the axes which should
    be normalized across. By default this is all but the first dimension.
    For time series data use `slice(2, None)` to average over the none Batch
    and Time data.
* `create_scale`: Boolean representing whether to create a trainable scale per
    channel applied after the normalization.
* `create_offset`: Boolean representing whether to create a trainable offset
    per channel applied after normalization and scaling.
* `eps`: Small epsilon to add to the variance to avoid division by zero.
    Defaults to 1e-4.
* `scale_init`: Optional initializer for the scale variable. Can only be set
    if with_scale is True. By default scale is initialized to one.
* `offset_init`: Optional initializer for the offset variable. Can only be set
    if with_offset is True. By default offset is initialized to zero.
* `data_format`: The data format of the input. Can be either `channels_first`,
    `channels_last`, `N...C` or `NC...`. By default it is `channels_last`.
* `name`: Name of the module.


#### [`GroupNorm.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#GroupNorm.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`GroupNorm.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#GroupNorm.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`GroupNorm.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#GroupNorm.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`GroupNorm.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#GroupNorm.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`GroupNorm.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#GroupNorm.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`GroupNorm.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#GroupNorm.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class InstanceNorm`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/axis_norm.py?q=class:InstanceNorm){#InstanceNorm .code-reference}

Normalizes inputs along the channel dimension.

See `AxisNorm` for more details.

Attributes:
  scale: If `create_scale`, a trainable variable holding the current scale
    after the module is connected for the first time.
  offset: If `create_offset`, a trainable variable holding the current offset
    after the module is connected for the first time.

#### [`InstanceNorm.__init__(create_scale, create_offset, eps=0.0001, scale_init=None, offset_init=None, data_format='channels_last', name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/axis_norm.py?l=255){#InstanceNorm.__init__ .code-reference}

Constructs an InstanceNorm module.

This method creates a module which normalizes over the channel dimension.

##### Args:


* `create_scale`: Boolean representing whether to create a trainable scale per
    channel applied after the normalization.
* `create_offset`: Boolean representing whether to create a trainable offset
    per channel applied after normalization and scaling.
* `eps`: Small epsilon to avoid division by zero variance. Defaults to 1e-4.
* `scale_init`: Optional initializer for the scale variable. Can only be set
    if `create_scale` is True. By default scale is initialized to one.
* `offset_init`: Optional initializer for the offset variable. Can only be set
    if `create_offset` is True. By default offset is initialized to zero.
* `data_format`: The data format of the input. Can be either `channels_first`,
    `channels_last`, `N...C` or `NC...`. By default it is `channels_last`.
* `name`: Name of the module.

##### Returns:

  An AxisNorm module.


#### [`InstanceNorm.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#InstanceNorm.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`InstanceNorm.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#InstanceNorm.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`InstanceNorm.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#InstanceNorm.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`InstanceNorm.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#InstanceNorm.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`InstanceNorm.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#InstanceNorm.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`InstanceNorm.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#InstanceNorm.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class LSTM`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?q=class:LSTM){#LSTM .code-reference}

Long short-term memory (LSTM) RNN core.

The implementation is based on: http://arxiv.org/abs/1409.2329.
Given x_t and the previous state (h_{t-1}, c_{t-1}) the core computes

    i_t = sigm(W_{ii} x_t + W_{hi} h_{t-1} + b_i)
    f_t = sigm(W_{if} x_t + W_{hf} h_{t-1} + b_f)
    g_t = tanh(W_{ig} x_t + W_{hg} h_{t-1} + b_g)
    o_t = sigm(W_{io} x_t + W_{ho} h_{t-1} + b_o)
    c_t = f_t c_{t-1} + i_t g_t
    h_t = o_t tanh(c_t)

Where `i_t`, `f_t`, `o_t` are input, forget and output gate activations,
and `g_t` is a vector of cell updates.

Following http://proceedings.mlr.press/v37/jozefowicz15.pdf we add a
constant `forget_bias` (defaults to 1.0) to `b_f` in order to reduce
the scale of forgetting in the beginning of the training.

#### Recurrent projections

Hidden state could be projected (via the `project_size` parameter)
to reduce the number of parameters and speed up computation. For more
details see https://arxiv.org/abs/1402.1128.

Variables:
  w_i: input-to-hidden weights `W_{ii}`, `W_{if}`, `W_{ig}` and `W_{io}`
    concatenated into a `tf.Tensor` of shape `[input_size, 3 * hidden_size]`.
  w_h: hidden-to-hidden weights `W_{hi}`, `W_{hf}`, `W_{hg}` and `W_{ho}`
    concatenated into a `tf.Tensor` of shape `[hidden_size, 3 * hidden_size]`.
  b: biases `b_i`, `b_f`, `b_g` and `b_o` concatenated into a `tf.Tensor` of
    shape `[3 * hidden_size]`.

#### [`LSTM.__init__(hidden_size, projection_size=None, projection_init=None, w_i_init=None, w_h_init=None, b_init=None, forget_bias=1.0, dtype=tf.float32, name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=734){#LSTM.__init__ .code-reference}

Construct an `LSTM`.

##### Args:


* `hidden_size`: Hidden layer size.
* `projection_size`: Optional int; if set, then the hidden state is
    projected to this size via a trainable projection matrix.
* `projection_init`: Optional initializer for the projection matrix.
    Defaults to `TruncatedNormal` with a standard deviation of
    `1 / sqrt(hidden_size).
* `w_i_init`: Optional initializer for the input-to-hidden weights.
    Defaults to `TruncatedNormal` with a standard deviation of
    `1 / sqrt(input_size).
* `w_h_init`: Optional initializer for the hidden-to-hidden weights.
    Defaults to `TruncatedNormal` with a standard deviation of
    `1 / sqrt(hidden_size).
* `b_init`: Optional initializer for the biases. Defaults to
    `Zeros`.
* `forget_bias`: Optional float to add to the bias of the forget gate
    after initialization.
* `dtype`: Optional `tf.DType` of the core's variables. Defaults to
    `tf.float32`.
* `name`: Name of the module.


#### [`LSTM.hidden_to_hidden`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=67){#LSTM.hidden_to_hidden .code-reference}




#### [`LSTM.initial_state(*args, **kwargs)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=60){#LSTM.initial_state .code-reference}

See base class.


#### [`LSTM.input_to_hidden`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=67){#LSTM.input_to_hidden .code-reference}




#### [`LSTM.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#LSTM.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`LSTM.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#LSTM.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`LSTM.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#LSTM.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`LSTM.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#LSTM.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`LSTM.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#LSTM.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`LSTM.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#LSTM.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class LSTMState`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?q=class:LSTMState){#LSTMState .code-reference}

LSTMState(hidden, cell)

#### `LSTMState.cell`{#LSTMState.cell .code-reference}

Alias for field number 1


#### `LSTMState.hidden`{#LSTMState.hidden .code-reference}

Alias for field number 0



### [`class LayerNorm`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/axis_norm.py?q=class:LayerNorm){#LayerNorm .code-reference}

Normalizes inputs along the spatial and channel dimensions.

See `AxisNorm` for more details.

Attributes:
  scale: If `create_scale`, a trainable variable holding the current scale
    after the module is connected for the first time.
  offset: If `create_offset`, a trainable variable holding the current offset
    after the module is connected for the first time.

#### [`LayerNorm.__init__(create_scale, create_offset, eps=0.0001, scale_init=None, offset_init=None, data_format='channels_last', name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/axis_norm.py?l=207){#LayerNorm.__init__ .code-reference}

Constructs an `LayerNorm` module.

This method creates a module which normalizes over the spatial and channel
dimensions.

##### Args:


* `create_scale`: Boolean representing whether to create a trainable scale per
    channel applied after the normalization.
* `create_offset`: Boolean representing whether to create a trainable offset
    per channel applied after normalization and scaling.
* `eps`: Small epsilon to avoid division by zero variance. Defaults to 1e-4.
* `scale_init`: Optional initializer for the scale variable. Can only be set
    if `create_scale` is True. By default scale is initialized to one.
* `offset_init`: Optional initializer for the offset variable. Can only be set
    if `create_offset` is True. By default offset is initialized to zero.
* `data_format`: The data format of the input. Can be either `channels_first`,
    `channels_last`, `N...C` or `NC...`. By default it is `channels_last`.
* `name`: Name of the module.

##### Returns:

  An AxisNorm module.


#### [`LayerNorm.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#LayerNorm.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`LayerNorm.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#LayerNorm.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`LayerNorm.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#LayerNorm.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`LayerNorm.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#LayerNorm.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`LayerNorm.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#LayerNorm.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`LayerNorm.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#LayerNorm.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class Linear`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/linear.py?q=class:Linear){#Linear .code-reference}

Linear module, optionally including bias.

#### [`Linear.__init__(output_size, with_bias=True, w_init=None, b_init=None, name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/linear.py?l=34){#Linear.__init__ .code-reference}

Constructs a `Linear` module.

##### Args:


* `output_size`: Output dimensionality.
* `with_bias`: Whether to include bias parameters. Default `True`.
* `w_init`: Optional initializer for the weights. By default the weights are
    initialized truncated random normal values with a standard deviation of
    `1 / sqrt(input_feature_size)`, which is commonly used when the inputs
    are zero centered (see https://arxiv.org/abs/1502.03167v3).
* `b_init`: Optional initializer for the bias. By default the bias is
    initialized to zero.
* `name`: Name of the module.


#### [`Linear.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#Linear.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`Linear.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#Linear.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`Linear.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#Linear.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`Linear.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#Linear.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Linear.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#Linear.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Linear.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#Linear.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class Metric`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/metrics.py?q=class:Metric){#Metric .code-reference}

Metric base class.

#### [`Metric.__init__(name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/base.py?l=281){#Metric.__init__ .code-reference}




#### [`Metric.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#Metric.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`Metric.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#Metric.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`Metric.reset(*args, **kwargs)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/metrics.py?l=60){#Metric.reset .code-reference}

Resets the metric.


#### [`Metric.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#Metric.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`Metric.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#Metric.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Metric.update(*args, **kwargs)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/metrics.py?l=60){#Metric.update .code-reference}

Accumulates values.


#### [`Metric.value`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/metrics.py?l=67){#Metric.value .code-reference}

Returns the current value of the metric.


#### [`Metric.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#Metric.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Metric.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#Metric.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class Module`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/base.py?q=class:Module){#Module .code-reference}

Base class for Sonnet modules.

A Sonnet module is a lightweight container for variables and other modules.
Modules typically define one or more "forward" methods (e.g. `__call__`) which
apply operations combining user input and module parameters. For example:

    >>> class MultiplyModule(snt.Module):
    ...   def __call__(self, x):
    ...     if not hasattr(self, 'w'):
    ...       self.w = tf.Variable(2., name='w')
    ...     return x * self.w

    >>> mod = MultiplyModule()
    >>> mod(1.)
    <tf.Tensor: ... numpy=2.0>

#### [`Module.__init__(name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/base.py?l=281){#Module.__init__ .code-reference}




#### [`Module.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#Module.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`Module.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#Module.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`Module.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#Module.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`Module.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#Module.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Module.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#Module.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Module.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#Module.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class RNNCore`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?q=class:RNNCore){#RNNCore .code-reference}

Base class for Recurrent Neural Network cores.

This class defines the basic functionality that every core should
implement: `initial_state` ,used to construct an example of the core
state; and `__call__` which applies the core parameterized by a
previous state to an input.

Cores are typically used with `snt.*_unroll` to iteratively construct
an output sequence from the given input sequence.

#### [`RNNCore.__init__(name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/base.py?l=281){#RNNCore.__init__ .code-reference}




#### [`RNNCore.initial_state(*args, **kwargs)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=60){#RNNCore.initial_state .code-reference}

Construct an initial state for this core.

##### Args:


* `batch_size`: An int or an integral scalar tensor representing
    batch size.
* `**kwargs`: Optional keyword arguments.

##### Returns:

  Arbitrarily nested initial state for this core.


#### [`RNNCore.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#RNNCore.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`RNNCore.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#RNNCore.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`RNNCore.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#RNNCore.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`RNNCore.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#RNNCore.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`RNNCore.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#RNNCore.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`RNNCore.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#RNNCore.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class Reshape`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/reshape.py?q=class:Reshape){#Reshape .code-reference}

Reshapes input Tensor, preserving the batch dimension.

For example, given an input Tensor with shape `[B, H, W, C, D]`:

    >>> B, H, W, C, D = range(1, 6)
    >>> x = tf.ones([B, H, W, C, D])

The default behavior when `output_shape` is (-1, D) is to flatten all
dimensions between `B` and `D`:

    >>> mod = snt.Reshape(output_shape=(-1, D))
    >>> assert mod(x).shape == [B, H*W*C, D]

You can change the number of preserved leading dimensions via
`preserve_dims`:

    >>> mod = snt.Reshape(output_shape=(-1, D), preserve_dims=2)
    >>> assert mod(x).shape == [B, H, W*C, D]

    >>> mod = snt.Reshape(output_shape=(-1, D), preserve_dims=3)
    >>> assert mod(x).shape == [B, H, W, C, D]

    >>> mod = snt.Reshape(output_shape=(-1, D), preserve_dims=4)
    >>> assert mod(x).shape == [B, H, W, C, 1, D]

#### [`Reshape.__init__(output_shape, preserve_dims=1, name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/reshape.py?l=175){#Reshape.__init__ .code-reference}

Constructs a Reshape module.

##### Args:


* `output_shape`: Shape to reshape the input Tensor to while preserving its
      first `preserve_dims` dimensions. When the special value -1
      appears in `output_shape` the corresponding size is automatically
      inferred. Note that -1 can only appear once in `output_shape`. To
      flatten all non-batch dimensions use `snt.Flatten`.
* `preserve_dims`: Number of leading dimensions that will not be reshaped.
* `name`: Name of the module.

##### Raises:


* `ValueError`: If `preserve_dims <= 0`.


#### [`Reshape.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#Reshape.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`Reshape.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#Reshape.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`Reshape.reversed(name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/reshape.py?l=223){#Reshape.reversed .code-reference}

Returns inverse batch reshape.


#### [`Reshape.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#Reshape.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`Reshape.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#Reshape.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Reshape.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#Reshape.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Reshape.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#Reshape.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class Sequential`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/sequential.py?q=class:Sequential){#Sequential .code-reference}

Sequential applies a linear chain of modules / callables.

    >>> mlp = snt.Sequential([
    ...     snt.Linear(1024),
    ...     tf.nn.relu,
    ...     snt.Linear(10),
    ... ])
    >>> mlp(tf.random.normal([8, 100]))
    <tf.Tensor: ...>

Note that `Sequential` is limited in the range of possible architectures
it can handle. This is a deliberate design decision; `Sequential` is only
meant to be used for the simple case of fusing together modules/ops where
the input of a particular module/op is the output of the previous one.

Another restriction is that it is not possible to have extra arguments in the
`__call__` method that are passed to the constituents of the module - for
example, if there is a `BatchNorm` module in `Sequential` and the user wishes
to switch the `is_training` flag. If this is the desired use case, the
recommended solution is to subclass `snt.Module` and implement `__call__`:

    >>> class CustomModule(snt.Module):
    ...   def __init__(self, name=None):
    ...     super(CustomModule, self).__init__(name=name)
    ...     self.conv2d = snt.Conv2D(32, 4, 2)
    ...     self.bn = snt.BatchNorm()
    ...
    ...   def __call__(self, inputs, is_training):
    ...     outputs = self.conv2d(inputs)
    ...     outputs = self.bn(outputs, is_training=is_training)
    ...     outputs = tf.nn.relu(outputs)
    ...     return outputs

#### [`Sequential.__init__(layers=None, name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/sequential.py?l=60){#Sequential.__init__ .code-reference}




#### [`Sequential.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#Sequential.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`Sequential.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#Sequential.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`Sequential.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#Sequential.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`Sequential.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#Sequential.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Sequential.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#Sequential.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`Sequential.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#Sequential.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class TrainableState`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?q=class:TrainableState){#TrainableState .code-reference}

Trainable state for an `RNNCore`.

The state can be constructed manually from a nest of initial values

    snt.TrainableState((tf.zeros([16]), tf.zeros([16])))

or automatically for a given `RNNCore`

    core = snt.LSTM(hidden_size=16)
    snt.TrainableState.for_core(core)

#### [`TrainableState.__init__(initial_values, mask=None, name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=116){#TrainableState.__init__ .code-reference}

Construct a trainable state from initial values.

##### Args:


* `initial_values`: Arbitrarily nested initial values for the state.
* `mask`: Optional boolean mask of the same structure as `initial_values`
    specifying which components should be trainable. If not given,
    the whole state is considered trainable.
* `name`: Name of the module.


#### [`TrainableState.for_core(cls, core, mask=None, name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=97){#TrainableState.for_core .code-reference}

Construct a trainable state for a given `RNNCore`.

##### Args:


* `core`: `RNNCore` to construct the state for.
* `mask`: Optional boolean mask of the same structure as the initial
    state of `core` specifying which components should be trainable.
    If not given, the whole state is considered trainable.
* `name`: Name of the module.

##### Returns:

  A `TrainableState`.


#### [`TrainableState.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#TrainableState.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`TrainableState.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#TrainableState.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`TrainableState.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#TrainableState.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`TrainableState.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#TrainableState.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`TrainableState.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#TrainableState.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`TrainableState.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#TrainableState.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`class VanillaRNN`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?q=class:VanillaRNN){#VanillaRNN .code-reference}

Basic fully-connected RNN core.

Given x_t and the previous hidden state h_{t-1} the core computes

    h_t = w_i x_t + w_h h_{t-1} + b

Variables:
  input_to_hidden/w: weights w_i, a `tf.Tensor` of shape
    `[input_size, hidden_size]`.
  hidden_to_hidden/w: weights w_h, a `tf.Tensor` of shape
    `[input_size, hidden_size]`.
  b: bias, a `tf.Tensor` or shape `[hidden_size]`.

#### [`VanillaRNN.__init__(hidden_size, activation=tanh, w_i_init=None, w_h_init=None, b_init=None, dtype=tf.float32, name=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=426){#VanillaRNN.__init__ .code-reference}

Construct a vanilla RNN core.

##### Args:


* `hidden_size`: Hidden layer size.
* `activation`: Activation function to use. Defaults to `tf.tanh`.
* `w_i_init`: Optional initializer for the input-to-hidden weights.
    Defaults to `TruncatedNormal` with a standard deviation of
    `1 / sqrt(input_size).
* `w_h_init`: Optional initializer for the hidden-to-hidden weights.
    Defaults to `TruncatedNormal` with a standard deviation of
    `1 / sqrt(hidden_size).
* `b_init`: Optional initializer for the bias. Defaults to `Zeros`.
* `dtype`: Optional `tf.DType` of the core's variables. Defaults to
    `tf.float32`.
* `name`: Name of the module.


#### [`VanillaRNN.hidden_to_hidden`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=67){#VanillaRNN.hidden_to_hidden .code-reference}




#### [`VanillaRNN.initial_state(*args, **kwargs)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=60){#VanillaRNN.initial_state .code-reference}

See base class.


#### [`VanillaRNN.input_to_hidden`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=67){#VanillaRNN.input_to_hidden .code-reference}




#### [`VanillaRNN.name`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=119){#VanillaRNN.name .code-reference}

Returns the name of this module as passed or determined in the ctor.

NOTE: This is not the same as the `self.name_scope.name` which includes
parent module names.


#### [`VanillaRNN.name_scope`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=128){#VanillaRNN.name_scope .code-reference}

Returns a `tf.name_scope` instance for this class.


#### [`VanillaRNN.submodules`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=164){#VanillaRNN.submodules .code-reference}

Sequence of all sub-modules.

Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).

```
a = tf.Module()
b = tf.Module()
c = tf.Module()
a.b = b
b.c = c
assert list(a.submodules) == [b, c]
assert list(b.submodules) == [c]
assert list(c.submodules) == []
```

##### Returns:

  A sequence of all submodules.


#### [`VanillaRNN.trainable_variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=149){#VanillaRNN.trainable_variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`VanillaRNN.variables`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=134){#VanillaRNN.variables .code-reference}

Sequence of variables owned by this module and it's submodules.

Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change.

##### Returns:

  A sequence of variables for the current module (sorted by attribute
  name) followed by variables from all submodules recursively (breadth
  first).


#### [`VanillaRNN.with_name_scope(cls, method)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/module/module.py?l=252){#VanillaRNN.with_name_scope .code-reference}

Decorator to automatically enter the module name scope.

```
class MyModule(tf.Module):
  @tf.Module.with_name_scope
  def __call__(self, x):
    if not hasattr(self, 'w'):
      self.w = tf.Variable(tf.random.normal([x.shape[1], 64]))
    return tf.matmul(x, self.w)
```

Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
names included the module name:

```
mod = MyModule()
mod(tf.ones([8, 32]))
# ==> <tf.Tensor: ...>
mod.w
# ==> <tf.Variable ...'my_module/w:0'>
```

##### Args:


* `method`: The method to wrap.

##### Returns:

  The original method wrapped such that it enters the module's name scope.



### [`deep_rnn_with_residual_connections(layers, name='deep_rnn_with_residual_connections')`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=653){#deep_rnn_with_residual_connections .code-reference}

Construct a `DeepRNN` with residual connections.

Residual connections alter the dependency structure in a `DeepRNN`.
Specifically, the input to the i-th intermediate layer is a sum of
the original core's inputs and the outputs of all the preceding
layers (<i).

    outputs0, ... = layers[0](inputs, ...)
    outputs0 += inputs
    outputs1, ... = layers[1](outputs0, ...)
    outputs1 += outputs0
    outputs2, ... = layers[2](outputs1, ...)
    outputs2 += outputs1
    ...

This allows the layers to learn specialized features that compose
incrementally.

##### Args:


* `layers`: A list of `RNNCore`s.
* `name`: Name of the module.

##### Returns:

  A `DeepRNN` with residual connections.

##### Raises:


* `ValueError`: If any of the layers is not an `RNNCore`.


### [`deep_rnn_with_skip_connections(layers, concat_final_output=True, name='deep_rnn_with_skip_connections')`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=589){#deep_rnn_with_skip_connections .code-reference}

Construct a `DeepRNN` with skip connections.

Skip connections alter the dependency structure within a `DeepRNN`.
Specifically, input to the i-th layer (i > 0) is given by a
concatenation of the core's inputs and the outputs of the (i-1)-th layer.

    outputs0, ... = layers[0](inputs, ...)
    outputs1, ... = layers[1](tf.concat([inputs, outputs0], axis=1], ...)
    outputs2, ... = layers[2](tf.concat([inputs, outputs1], axis=1], ...)
    ...

This allows the layers to learn decoupled features.

##### Args:


* `layers`: A list of `RNNCore`s.
* `concat_final_output`: If enabled (default), the outputs of the core
    is a concatenation of the outputs of all intermediate layers;
    otherwise, only the outputs of the final layer, i.e. that of
    `layers[-1]`, are returned.
* `name`: Name of the module.

##### Returns:

  A `DeepRNN` with skip connections.

##### Raises:


* `ValueError`: If any of the layers is not an `RNNCore`.


### [`dynamic_unroll(core, input_sequence, initial_state, sequence_length=None, parallel_iterations=1, swap_memory=False)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=246){#dynamic_unroll .code-reference}

Perform a dynamic unroll of an RNN.

    >>> core = snt.LSTM(hidden_size=16)
    >>> batch_size = 3
    >>> input_sequence = tf.random.uniform([1, batch_size, 2])
    >>> output_sequence, final_state = snt.dynamic_unroll(
    ...     core,
    ...     input_sequence,
    ...     core.initial_state(batch_size))

An *unroll* corresponds to calling an RNN on each element of the
input sequence in a loop, carrying the state through:

    state = initial_state
    for t in range(len(input_sequence)):
       outputs, state = core(input_sequence[t], state)

A *dynamic* unroll preserves the loop structure when executed within
`tf.function`. See `snt.static_unroll` for an unroll function which
replaces a loop with its body repeated multiple times.

##### Args:


* `core`: An `RNNCore` to unroll.
* `input_sequence`: An arbitrarily nested structure of tensors of shape
    `[T, B, ...]` where T is the number of time steps, and B is the
    batch size.
* `initial_state`: initial state of the given core.
* `sequence_length`: An optional tensor of shape `[B]` specifying the
    lengths of sequences within the (padded) batch.
* `parallel_iterations`: An optional int specifying the number of
    iterations to run in parallel. Those operations which do not have
    any temporal dependency and can be run in parallel, will be. This
    parameter trades off time for space. Values >> 1 use more memory
    but take less time, while smaller values use less memory but
    computations take longer. Defaults to 1.
* `swap_memory`: Transparently swap the tensors produced in forward
    inference but needed for back prop from GPU to CPU. This allows
    training RNNs which would typically not fit on a single GPU,
    with very minimal (or no) performance penalty. Defaults to False.

##### Returns:


* `output_sequence`: An arbitrarily nested structure of tensors of shape
    `[T, B, ...]`. Dimensions following the batch size could be
    different from that of `input`.
* `final_state`: Core state at time step T.

##### Raises:


* `ValueError`: if `input_sequence` is empty.


### [`lstm_with_recurrent_dropout(hidden_size, dropout=0.5, seed=None, **kwargs)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=1014){#lstm_with_recurrent_dropout .code-reference}

Construct an LSTM with recurrent dropout.

The implementation is based on https://arxiv.org/abs/1512.05287.
Dropout is applied on the previous hidden state `h_{t-1}` during the
computation of gate activations

    i_t = sigm(W_{ii} x_t + W_{hi} d(h_{t-1}) + b_i)
    f_t = sigm(W_{if} x_t + W_{hf} d(h_{t-1}) + b_f)
    g_t = tanh(W_{ig} x_t + W_{hg} d(h_{t-1}) + b_g)
    o_t = sigm(W_{io} x_t + W_{ho} d(h_{t-1}) + b_o)

##### Args:


* `hidden_size`: Hidden layer size.
* `dropout`: Dropout probability.
* `seed`: Optional int; seed passed to `tf.nn.dropout`.
* `**kwargs`: Optional keyword arguments to pass to the `LSTM` constructor.

##### Returns:


* `train_lstm`: an `LSTM` with recurrent dropout enabled for training.
* `test_lstm`: the same as `train_lstm` but without recurrent dropout.

##### Raises:


* `ValueError`: If `dropout` is not in `[0, 1)`.


### [`no_name_scope(method)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/base.py?l=36){#no_name_scope .code-reference}

Decorator to wrap a method, preventing automatic name scope wrapping.

By default, any method on a module is considered as a forwards function, and
so any variables / modules created by the method will be scoped as belonging
to the module. In some cases this is undesirable, for example when
implementing `.clone()` / `.transpose()`, as in those cases we want the new
module to have the scope of wherever the `.transpose()` call is made. To
allow this, decorate any methods with `no_module_name_scope`.

This logic is tied to `ModuleMetaclass.__new__`, if anything is
changed here corresponding changes will be needed there.

##### Args:


* `method`: the method to wrap.

##### Returns:

  The method, with a flag indicating no name scope wrapping should occur.


### [`once(f)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/once.py?l=29){#once .code-reference}

Decorator which ensures a wrapped method is only ever run once.

    >>> @once
    ... def f():
    ...   print('Hello, world!')
    >>> f()
    Hello, world!
    >>> f()
    >>> f()

If `f` is a method then it will be evaluated once per instance:

    >>> class MyObject(object):
    ...   @once
    ...   def f(self):
    ...     print('Hello, world!')

    >>> o = MyObject()
    >>> o.f()
    Hello, world!
    >>> o.f()

    >>> o2 = MyObject()
    >>> o2.f()
    Hello, world!
    >>> o.f()
    >>> o2.f()

If an error is raised during execution of `f` it will be raised to the user.
Next time the method is run, it will be treated as not having run before.

##### Args:


* `f`: A function to wrap which should only be called once.

##### Returns:

  Wrapped version of `f` which will only evaluate `f` the first time it is
  called.


### [`static_unroll(core, input_sequence, initial_state, sequence_length=None)`](https://github.com/deepmind/sonnet/blob/v2/sonnet/src/recurrent.py?l=153){#static_unroll .code-reference}

Perform a static unroll of an RNN.

    >>> core = snt.LSTM(hidden_size=16)
    >>> batch_size = 3
    >>> input_sequence = tf.random.uniform([1, batch_size, 2])
    >>> output_sequence, final_state = snt.static_unroll(
    ...     core,
    ...     input_sequence,
    ...     core.initial_state(batch_size))

An *unroll* corresponds to calling an RNN on each element of the
input sequence in a loop, carrying the state through:

    state = initial_state
    for t in range(len(input_sequence)):
       outputs, state = core(input_sequence[t], state)

A *static* unroll replaces a loop with its body repeated multiple
times when executed inside `tf.function`:

    state = initial_state
    outputs0, state = core(input_sequence[0], state)
    outputs1, state = core(input_sequence[1], state)
    outputs2, state = core(input_sequence[2], state)
    ...

See `snt.dynamic_unroll` for a loop-preserving unroll function.

##### Args:


* `core`: An `RNNCore` to unroll.
* `input_sequence`: An arbitrarily nested structure of tensors of shape
    `[T, B, ...]` where T is the number of time steps, and B is the
    batch size.
* `initial_state`: initial state of the given core.
* `sequence_length`: An optional tensor of shape `[B]` specifying the
    lengths of sequences within the (padded) batch.

##### Returns:


* `output_sequence`: An arbitrarily nested structure of tensors of shape
    `[T, B, ...]`. Dimensions following the batch size could be
    different from that of `input`.
* `final_state`: Core state at time step T.

##### Raises:


* `ValueError`: if `input_sequence` is empty.


